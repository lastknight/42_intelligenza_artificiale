# <a name="_toc138674423"></a>**INTELLIGENZA ARTIFICIALE**
#### **IL REGOLAMENTO EUROPEO E IL RISPETTO DEI DIRITTI SPIEGATO ALLE AZIENDE**
Lucia Maggi e Giuseppe Vaciago 

w

[INTELLIGENZA ARTIFICIALE	1****](#_toc138674423)**

[**Executive Summary – Regolamento AI	2****](#_toc138674424)

[**Che cosa prevede l’AI Act?	2****](#_toc138674425)

[**Executive Summary – Tutela del copyright nel contesto dell’AI	3****](#_toc138674426)

[**1. Introduzione: il regolamento in pillole	5****](#_toc138674427)

[**2. A chi si applica il Regolamento sull’intelligenza artificiale	7****](#_toc138674428)

[**3. Obblighi per i modelli di base e trasversali	7****](#_toc138674429)

[3.1 Obblighi per i modelli di base (GPT)	7](#_toc138674430)

[3.2 Obblighi di trasparenza trasversali	8](#_toc138674431)

[**4. Obblighi specifici per sistemi ad alto rischio	10****](#_toc138674432)

[4.1 Obblighi relativi alla gestione del rischio	10](#_toc138674433)

[4.2 Obblighi relativi alla governance dei dati	11](#_toc138674434)

[4.3 Obblighi specifici di trasparenza	12](#_toc138674435)

[4.4 Obbligo di supervisione	13](#_toc138674436)

[4.5 Obblighi di conformità ai requisiti europei	15](#_toc138674437)

[4.6 Obblighi di interazione e collaborazione con le Autorità	15](#_toc138674438)

[4.7 Obblighi di accuratezza, robustezza, cybersicurezza e resilienza	17](#_toc138674439)

[**AI e contenuti creativi: una breve introduzione	21****](#_toc138674440)

[**I rischi connessi all’utilizzo di un’opera generata da intelligenza artificiale	23****](#_toc138674441)

[6.1 Il prodotto di un sistema di AI generativa può essere protetto dal diritto d’autore?	24](#_toc138674442)

[6.2. Quali sono i soggetti autorizzati a sfruttare i prodotti generati da AI?	27](#_toc138674443)

[6.3. È possibile che il prodotto generato mediante un software di AI violi i diritti di proprietà di terzi?	29](#_toc138674444)




## **Prefazione**
Questa è una bella prefazione
## <a name="_toc138674424"></a>**Executive Summary – <a name="_toc138674425"></a>Che cosa prevede l’AI Act?**
L’AI Act è una normativa che ha l’ambizione di fornire un framework normativo sia per chi realizza intelligenza artificiale sia per chi la utilizza. La Proposta normativa si basa su un sistema di classificazione volto a determinare il **livello di rischio** che una tecnologia di AI potrebbe rappresentare per la salute, la sicurezza e i diritti fondamentali delle persone.

Se volessimo sintetizzarla in cinque punti potremmo dire che: 

1. Come la normativa a tutela dei dati personali (GDPR), questa è una normativa risk based dove i vari player (fornitori di AI e utilizzatori di AI) dovranno fare una **valutazione di impatto** per determinare la legittimità dello strumento di intelligenza artificiale usato. La valutazione di impatto non sarà solo sui profili privacy e cybersecurity, ma anche e soprattutto sulla possibile lesione dei diritti umani e sugli impatti etici e sociali.
1. Il regolamento divide tra intelligenze artificiale che creano: (i) un rischio **inaccettabile**; (ii) un rischio **medio-basso**; (iii) un rischio **alto**. A seconda del livello di rischio si prevedono più o meno elevati obblighi di conformità.
1. Il regolamento lascia aperta la porta a **nuove applicazioni di intelligenza artificiale** attraverso una normazione che rimanda all’allegato III per definire cosa rientri nel concetto di alto rischio per i diritti dei cittadini europei. 
1. L’intelligenza artificiale generativa e quindi **CHATGPT** è stata inserita all’ultimo grazie all’ottimo lavoro del nostro Europarlamentare Brando Benifei, perché sostanzialmente prima non era stata normata. Sotto questo profilo, il Regolamento è molto severo sui dati che un sistema di AI generativa utilizza per rispondere ai quesiti (dataset). Ne consegue che se la normativa fosse già in vigore, CHATGPT avrebbe seri problemi ad essere uno strumento legittimo in Europa. 
1. I tempi per l’approvazione definitiva, dopo la consultazione con Consiglio d’Europa e Commissione, dovrebbero essere approvati per la fine dell’anno. I tempi l’entrata in vigore, tuttavia, **sono 24 mesi** dall’approvazione e quindi, qualora non ci fossero intoppi, dovrebbe entrare in vigore a fine **2025** o, al più tardi, all’inizio del **2026**. 

Che cosa cambierà per le imprese che usano l’AI? 

L’AI avrà degli impatti molto significativi, perché gli obblighi non riguardano solo **chi la realizza**, ma anche **chi la utilizza**. Quindi le imprese che utilizzeranno AI ad alto rischio, dovranno valutare con molta attenzione gli strumenti di intelligenza artificiale di cui vogliono dotarsi. 

A seguito di un'analisi dell’AI Act sono emerse, nello specifico, **sette macrocategorie di obblighi**: (i) obblighi relativi alla gestione del rischio, (ii) obblighi relativi alla gestione dei dati, (iii) obblighi di trasparenza, (iv) obblighi di supervisione, (v) obblighi di conformità ai requisiti europei, (vi) obblighi di collaborazione con le Autorità e (vii) obblighi di accuratezza, cybersicurezza e resilienza. 
## <a name="_toc138674426"></a>**Executive Summary – Tutela del copyright nel contesto dell’AI**
È innegabile che l'Intelligenza Artificiale (AI) abbia subito una rapida evoluzione nell'ultimo periodo, influenzando diversi ambiti, tra cui l'arte e la musica. Trattandosi di una nuova modalità di creazione di contenuti, è innegabile che nascano una serie di interrogativi legati alla natura dei contenuti ed alla titolarità dei diritti di proprietà intellettuale relativi ai contenuti stessi. 

1. Chi è l’autore di un’opera generata da AI? 
1. 2. Diritti e interessi creativi: fino a che punto si può sperimentare con AI senza mettere in discussione i diritti e gli interessi degli autori originali?
1. 3. Quali sono i rischi sotto il profilo del diritto d’autore per le aziende che decidono di utilizzare prodotti di software di Intelligenza Artificiale?

Comprendere queste questioni è fondamentale nel tentativo di stabilire i limiti e i pericoli nell'utilizzo dell'intelligenza artificiale per creare contenuti artistici.

Tuttavia risolvere questi problemi è complesso e, in molti casi, dipende dalle circostanze specifiche. **Gli enti preposti alla registrazione delle opere**, come il Copyright Office statunitense, **tendono a richiedere la paternità umana come requisito per la protezione del copyright** e dello stesso avviso sono anche il legislatore comunitario e quello nazionale italiano.

Le politiche di alcuni software regolamentano la proprietà e l'utilizzo commerciale delle opere generate in base ai termini del servizio e ai piani d'iscrizione degli utenti. La loro disamina è fondamentale per comprendere le possibili implicazioni legali nell'utilizzo di opere generate mediante l’utilizzo di AI.

I più recenti casi che hanno interessato i Tribunali statunitensi hanno evidenziato come l’uso di opere di terzi per l’attività di training dei software di AI espone al rischio concreto di richieste risarcitorie da parte dei soggetti titolari di diritti di proprietà intellettuale sulle opere utilizzate dalle piattaforme come dataset per l’addestramento dei software.

A questo proposito **l’AI act** ha previsto la possibilità di **richiedere ai fornitori** di sistemi di IA generativa **maggior trasparenza** e moderazione dei contenuti. 



## <a name="_toc138674427"></a>**1. Introduzione: il regolamento in pillole**
L’Artificial Intelligence Act (noto come AI Act) è il primo atto normativo organico al mondo sull’intelligenza artificiale che mira a istituire un quadro giuridico uniforme volto a regolare lo sviluppo, la commercializzazione e l’uso dei sistemi di intelligenza artificiale in conformità con i valori e i diritti costituzionali dell’UE, limitando i rischi e prevenendo gli eventuali danni.

Si presume **entrerà in vigore tra la fine del 2023 e l’inizio del 2024** con i consueti due anni di attesa prima della sua concreta applicazione che quindi avverrà tra la fine del 2025 o l’inizio del 2026. Il percorso normativo è iniziato il 21 aprile 2021, con la proposta avanzata dalla Commissione Europea. A seguire, nel dicembre 2022 anche il Consiglio dell’Unione ha assunto la sua posizione su questo tema, e ha voluto specificare come una normativa che disciplini AI nasca dall’esigenza di assicurare che i sistemi di AI immessi all’interno del mercato dell’Unione europea siano sicuri e rispettino la normativa vigente in materia di diritti fondamentali e i valori dell’Unione*.* 

La proposta normativa si basa su un sistema di classificazione volto a determinare il livello di rischio che una tecnologia di AI potrebbe rappresentare per la salute, la sicurezza e i diritti fondamentali delle persone, cd. risk-based approach, differenziando tra gli usi dell'AI che creano: (i) un rischio inaccettabile; (ii) un rischio medio basso; (iii) un rischio alto. A seconda del livello di rischio si prevedono più o meno elevati obblighi di conformità.

![Immagine che contiene Carattere, schermata, linea, testo

Descrizione generata automaticamente](Aspose.Words.86238f15-0794-4f42-a88c-1ff665e72b8c.001.png)

Per quanto concerne il **rischio inaccettabile**, viene individuato un elenco delle pratiche che comprende tutti i sistemi di AI il cui uso è contrario ai valori dell'Unione, ad esempio perché viola i diritti fondamentali. L'articolo 5 della Proposta di regolamento vieta, infatti, l'uso di tecniche subliminali, lo sfruttamento delle vulnerabilità di gruppi specifici, la valutazione discriminatoria delle persone e l'uso non autorizzato di identificazione biometrica in tempo reale. L'uso di identificazione biometrica richiede l’autorizzazione da un'Autorità indipendente.

**In linea generale** **per lo sviluppo ed uso di sistemi di intelligenza artificiale** responsabili ed etici sono stati individuati, all’articolo 4, **sei principi fondamentali**. Questi principi riguardano (i) la supervisione umana, (ii) la robustezza tecnica e la sicurezza, (iii) la privacy e la governance dei dati, (iv) la trasparenza, (v) la diversità, la non discriminazione e l'equità, ed infine (vi) il benessere sociale e ambientale. 

Vi sono poi **obblighi** che possono considerarsi trasversali, ovvero **destinati a particolari categorie di AI a prescindere dall’inquadramento del rischio** per tenere conto dei rischi specifici di manipolazione che essi comportano. L’intervento apportato dai relatori Benifei e Tudorache il 14 giugno 2023, è da considerarsi di particolare rilievo, in quanto ha introdotto un emendamento specifico relativo all’AI generativa, **come ChatGPT** in forza del quale tali sistemi devono rispettare determinati obblighi. 

Oltre a quelli previsti dal recente emendamento vi sono determinati obblighi di trasparenza, che si applicheranno ai sistemi che: (i) interagiscono con gli esseri umani; (ii) sono utilizzati per rilevare emozioni o stabilire un'associazione con categorie (sociali) sulla base di dati biometrici; oppure (iii) generano o manipolano contenuti ("deep fake"). 

Infine, vi sono gli obblighi previsti esclusivamente per le **AI ad alto rischio**. Per comprendere come classificare un AI di alto rischio è necessario consultare gli allegati II e III alla Proposta. La scelta è stata quella di non prevedere una rigida definizione di cosa si intenda per AI ad alto rischio e demandare ad un apposito allegato, in quanto sarebbe stato rischioso un elenco tassativo considerata la rapidità con cui i sistemi di intelligenza artificiale si evolvono.

Rientrano, infatti, in questa categoria le AI che (i) sono destinate ad essere utilizzate come componente di sicurezza di un prodotto, o sono loro stesse un prodotto regolato dalle norme elencate nell’Allegato II e sono soggette a una valutazione della conformità da parte di terzi per l’immissione sul mercato ai sensi dalle normative dell'allegato II, oppure (ii) si tratta di sistemi di IA elencati all’interno dell'allegato III di cui la Commissione fornirà successivamente orientamenti. 

A seguito di un'analisi dell’AI Act è possibile sintetizzare **sette macrocategorie di obblighi**: (i) obblighi relativi alla gestione del rischio, (ii) obblighi relativi alla gestione dei dati, (iii) obblighi di trasparenza, (iv) obblighi di supervisione, (v) obblighi di conformità ai requisiti europei, (vi) obblighi di collaborazione con le Autorità e (vii) obblighi di accuratezza, cybersicurezza e resilienza.
## <a name="_toc138674428"></a>**2. A chi si applica il Regolamento sull’intelligenza artificiale**
L’articolo 3 del Regolamento identifica una serie di soggetti che dovranno rispettare, in modo diverso i vari obblighi che elenchiamo di seguito:

- **fornitore:** una persona fisica o giuridica, un'autorità pubblica, un'agenzia o un altro organismo che sviluppa un sistema di IA o che fa sviluppare un sistema di IA al fine di immetterlo sul mercato;
- **fornitore di piccole dimensioni**: un fornitore che è una microimpresa o una piccola impresa ai sensi della raccomandazione 2003/361/CE della Commissione 61;
- **utente**: qualsiasi persona fisica o giuridica, autorità pubblica, agenzia o altro organismo che utilizza un sistema di IA sotto la sua autorità, tranne nel caso in cui il sistema di IA sia utilizzato nel corso di un'attività personale non professionale;
- **rappresentante autorizzato**: qualsiasi persona fisica o giuridica stabilita nell'Unione che ha ricevuto un mandato scritto da un fornitore di un sistema di IA al fine, rispettivamente, di adempiere ed eseguire per suo conto gli obblighi e le procedure stabiliti dal presente regolamento;
- **importatore**: qualsiasi persona fisica o giuridica stabilita nell'Unione che immette sul mercato o mette in servizio un sistema di IA recante il nome o il marchio di una persona fisica o giuridica stabilita al di fuori dell'Unione;
- **distributore:** qualsiasi persona fisica o giuridica nella catena di approvvigionamento, diversa dal fornitore o dall'importatore, che mette a disposizione un sistema di IA sul mercato dell'Unione senza modificarne le proprietà.

Nei prossimi paragrafi saranno divisi gli obblighi in base alle tre più rilevanti categorie di soggetti che dovranno applicare il regolamento: il f**ornitore**, l’**utente** e il **distributore**. Mentre è chiaro il ruolo dei primi due, in quanto il fornitore è chi produce o fa produrre sistemi di AI e l’utente è colui il quale utilizza tali sistemi, per il distributore, la questione interpretativa sul suo ruolo diventa più complessa.

Come è facile immaginarsi, l’utente avrà certamente obblighi minori anche se dovrà sempre tenere a mente i 6 principi fondanti il regolamento descritti nel precedente paragrafo e soprattutto dovrà effettuare le valutazioni di impatto ai sensi del GDPR ed eventualmente quella sui diritti umani. Inoltre, nel caso di utilizzo pervasivo di sistemi di Ai è opportuno che si organizzi per avere all’interno della sua azienda un comitato etico in grado di valutare gli impatti dei sistemi di AI.
## <a name="_toc138674429"></a>**3. Obblighi per i modelli di base e trasversali**
### <a name="_toc138674430"></a>**3.1 Obblighi per i modelli di base (GPT)**
Il Parlamento europeo ha inserito nuovi obblighi per i Fornitori di modelli di base di AI ovvero quei sistemi di AI pre-addestrati che possono essere utilizzati a loro volta come base per lo sviluppo di altri sistemi di AI. 

Il nuovo articolo 28(b) tra le altre cose richiede ai Fornitori di queste particolari tecnologie di:

- utilizzare **set di dati adeguati** a evitare degenerazioni;
- garantire **un'adeguata qualità** (prestazioni, prevedibilità, sicurezza, ecc.); 
- applicare gli standard di **efficienza energetica**. 

La vera innovazione introdotta però sta nella disciplina delle c.d. AI generative ovvero quelle AI come ChatGPT che sono in grado di generare testo, immagini, video, musica o altri media in risposta a delle richieste fatte. Proprio in luce della particolarità di queste tecnologie viene previsto che:

- si rispettino gli obblighi di trasparenza;
- vengano previste garanzie adeguate **contro la generazione di contenuti che violano il diritto dell'UE** in linea con lo stato dell'arte e senza pregiudicare i diritti fondamentali;
- venga documentata pubblicamente una sintesi dell'uso dei dati di addestramento protetti da copyright (come si vedrà nel capitolo dedicato).

La particolarità di questa disposizione è che non si era mai arrivati a delineare così nel dettaglio obblighi per questo tipo di tecnologia. La stessa proposta in un primo momento non aveva considerato l’importanza e la delicatezza di prevedere obblighi ad hoc per un’AI ormai di grande utilizzo come ChatGPT. 

Questo permetterà un impiego diverso e più consapevole essendo stata contemplata la possibilità di richiedere ai fornitori di sistemi di AI generativa maggior trasparenza e moderazione dei contenuti.
### <a name="_toc138674431"></a>**3.2 Obblighi di trasparenza trasversali**
Fornitori

I Fornitori di sistemi di che interagiscono con esseri umani devono: 

- informare in modo tempestivo, chiaro e comprensibile le persone che interagiscono con essi che ad interfacciarsi con loro è un’AI, a meno che ciò non sia già ovvio. 

Utenti

Gli Utenti di sistemi di riconoscimento delle emozioni o di categorizzazione biometrica devono:

- informare le persone esposte al sistema del trattamento dei loro dati biometrici e di altri dati personali; 
- ottenere il loro consenso. 

Gli Utenti di sistemi di AI che generano o manipolano contenuti c.d. "deep fake" devono:

- comunicare in modo appropriato e visibile che i contenuti sono stati generati o manipolati artificialmente;
- indicare il nome della persona fisica o giuridica che lo ha generato o manipolato. 

Le informazioni relative ai punti precedenti devono essere fornite alle persone fisiche al momento della prima interazione o esposizione. 

La trasparenza è un obbligo che ha un ruolo centrale nella normativa e riguarda tutti i trattamenti e i servizi basati sull’Intelligenza Artificiale, come dimostrato anche dai principi generali elencati all’interno dell’articolo 4. Anche per i sistemi che presentano un rischio limitato (come i deepfake) la Proposta di Regolamento non impone doveri particolari, ma prevede comunque limitati obblighi di trasparenza. 

Si tratta infatti di uno dei principi fondamentali non soltanto in tema di sviluppo di intelligenze artificiali, ma anche più in generale in tema di trattamento dei dati personali.

<table><tr><th colspan="4">PARTICOLARI CATEGORIE DI AI</th></tr>
<tr><td colspan="2" valign="top"><p>INTERAGISCONO CON GLI ESSERI UMANI</p><p>UTILIZZATI PER RILEVARE EMOZIONI O STABILIRE UN'ASSOCIAZIONE CON CATEGORIE SULLA BASE DI DATI BIOMETRICI</p><p>GENERANO O MANIPOLANO CONTENUTI.</p></td><td colspan="2"><p>DI BASE </p><p>GENERATIVE</p></td></tr>
<tr><td colspan="1">Art.</td><td colspan="1">Obblighi</td><td colspan="1">Art.</td><td colspan="1">Obblighi</td></tr>
<tr><td colspan="1" rowspan="2">52</td><td colspan="1" valign="top">(2) Gli utenti di un sistema di riconoscimento delle emozioni o di un sistema di categorizzazione biometrica <b>informano in modo tempestivo, chiaro e comprensibile del funzionamento del sistema le persone fisiche che vi sono esposte</b> e ottengono il loro consenso prima del trattamento dei loro dati biometrici e di altri dati personali.</td><td colspan="1" rowspan="2">28b</td><td colspan="1" valign="top">(2) i Fornitori devono per i modelli base di AI utilizzare <b>set di dati adeguati</b> per evitare degenerazioni, garantire un'adeguata qualità (prestazioni, prevedibilità, sicurezza, ecc.) ed applicare gli standard di efficienza energetica.</td></tr>
<tr><td colspan="1" valign="top">(3) Gli utenti di un sistema di AI che genera o manipola contenuti testuali, audio o visivi che sembrerebbero falsamente autentici o veritieri e che presentano rappresentazioni di persone che sembrano dire o fare cose che non hanno detto o fatto, senza il loro consenso ("deep fake"), <b>sono tenuti a comunicare in modo appropriato, tempestivo, chiaro e visibile che il contenuto è stato generato o manipolato artificialmente</b>, nonché, ove possibile, il nome della persona fisica o giuridica che lo ha generato o manipolato.</td><td colspan="1" valign="top">(3) per le AI generative sono previsti gli <b>obblighi di trasparenza, garanzie adeguate contro la generazione di contenuti che violano il diritto dell'UE</b> in linea con lo stato dell'arte e senza pregiudicare i diritti fondamentali e l’obbligo di documentazione dell'uso dei dati di addestramento protetti da copyright.</td></tr>
</table>
## <a name="_toc138674432"></a>**4. Obblighi specifici per sistemi ad alto rischio**
In questa sezione andremo ad analizzare gli obblighi specifichi che riguardano due distinte categorie. La prima è quella del fornitore e la seconda è quella dell’utente. L’articolo 3 del regolamento, come anticipato sopra, definisce queste due diverse categorie.
### <a name="_toc138674433"></a>**4.1 Obblighi relativi alla gestione del rischio**
**Fornitori**

Il fornitore deve:

- **creare** un **sistema di gestione del rischio** per poter valutare e adottare misure di sicurezza consone;
- **testare il sistema** per individuare quelle che sono le misure di gestione più appropriate e mirate con l’obiettivo di: (i) garantire l’eliminazione o riduzione dei rischi individuati; (ii) attuare misure di mitigazione e controllo per affrontare i rischi che non possono essere eliminati e (iii) fornire istruzioni d’uso comprensibili;
- valutare, nell'attuare il sistema di gestione del rischio, se il sistema di AI ad alto rischio possa avere o meno un **impatto negativo su gruppi di persone vulnerabili o minori**, riesaminandolo ed aggiornandolo regolarmente per garantirne l’efficacia continua.

**Utenti**

Gli Utenti devono:

- preparare una **valutazione d’impatto** **sui diritti fondamentali** prima di mettere sul mercato un sistema di AI ad alto rischio. La valutazione deve includere una serie di elementi tra cui:
- lo scopo d’uso; 
- l’area geografica; 
- le categorie di persone che potrebbero essere danneggiate;
- la conformità alle norme sui diritti fondamentali;
- l’impatto ambientale;
- il piano dettagliato su come vanno mitigati i danni e l’impatto negativo sui diritti fondamentali.
- **notificare l’inizio della valutazione** alle Autorità nazionali e ai rappresentanti delle parti più deboli (come le associazioni dei consumatori, gli organismi per la parità, i sindacati e le agenzie per la protezione dei dati) al fine di considerare tutte le possibili ripercussioni della tecnologia sui soggetti più vulnerabili;
- **effettuare una valutazione d’impatto sulla protezione dei dati**, conformemente a quanto previsto dal GDPR, e quindi valutare i rischi che un determinato trattamento di dati personali può avere sulla protezione dei dati stessi. 

### <a name="_toc138674434"></a>**4.2 Obblighi relativi alla governance dei dati**
**Fornitori**

I Fornitori devono indicare ogni tipo di informazione pertinente sul set di dati utilizzato, tenendo conto sia lo scopo previsto del sistema di AI, sia eventuali usi differenti - compresi quelli impropri - ragionevolmente prevedibili. Nello specifico, i dati utilizzati dai sistemi di AI ad alto rischio devono:

- essere soggetti ad una gestione adeguata che vada a disciplinare, conformemente allo scopo previsto dal sistema di AI, diversi aspetti a partire dalla **raccolta degli stessi** fino alla individuazione di misure appropriate per individuare, prevenire e attenuare eventuali distorsioni; 
- essere **pertinenti, rappresentativi, adeguati allo scopo** e la loro raccolta deve avvenire considerando lo specifico utilizzo del sistema di AI. 

I Fornitori possono **eccezionalmente utilizzare dati personali sensibili** solo se necessario **per individuare e correggere bias negativi** e solo se vengono rispettati determinati criteri tra cui: 

- la presenza di garanzie adeguate per i diritti e le libertà fondamentali;
- la pseudonimizzazione dei dati;
- l’adozione di misure tecniche e organizzative per garantire che i dati siano protetti e che solo le persone autorizzate vi abbiano accesso;
- la previsione della cancellazione una volta che l’errore viene corretto o il periodo di conservazione è scaduto;
- la redazione di una documentazione che spieghi perché il trattamento di categorie di dati personali sensibili era necessario.

**Utenti**

Anche gli utenti hanno l’obbligo di garantire che i dati di input siano **appropriati e sufficientemente rappresentativi** considerando la finalità prevista dal sistema di AI.

La gestione dei dati è fondamentale per prevenire la discriminazione e garantire l'uguaglianza, come dimostrano diversi casi di discriminazione di genere ed etnica posti in essere dai sistemi di AI negli ultimi anni. Queste problematiche sono state rese note a livello europeo per la prima volta con la pubblicazione del documento dell’UNESCO <i>I’d Blush If I Could</i></sup> seguito poi da una serie di studi e linee guida anche a livello nazionale. È proprio per questi problemi che il testo sottolinea l'importanza di adottare accorgimenti adeguati nella gestione dei dati e nell'utilizzo dei sistemi di AI. Ciò garantirà che i sistemi di AI ad alto rischio siano correttamente addestrati su dati pertinenti, rappresentativi e adeguati e che i pregiudizi siano individuati e corretti in modo efficace e responsabile.
### <a name="_toc138674435"></a>**4.3 Obblighi specifici di trasparenza** 
**Fornitori**

I Fornitori devono: 

- **redigere, conservare e aggiornare la documentazione tecnica** di tale sistema in modo da dimostrarne la conformità ai requisiti richiesti dalla presente Proposta prima di immettere sul mercato o mettere in servizio l’AI ad alto rischio;
- prevedere e conservare la **registrazione automatica dei log**, che devono contenere alcune informazioni minime, tra cui:
- data e ora di inizio di ogni utilizzo;
- data e ora di fine di ogni utilizzo;
- banca dati di riferimento;
- persone coinvolte nella verifica dei risultati.
- **fornire istruzioni** chiare, precise, corrette, oltre ad una serie di **informazioni** personali, quali: (i) il nome, (ii) la denominazione commerciale registrata o il marchio registrato, (iii) il proprio indirizzo, (iv) le informazioni di contatto.
- **informare** in modo chiaro, tempestivo e accessibile tutti i soggetti **di star comunicando con un sistema di AI.**

**Utenti**

Anche in capo agli Utenti è previsto un generale dovere di trasparenza che si articola in diversi obblighi:

- **conservare** per 6 mesi i **log di sistema** per controllarne il funzionamento, dimostrarne la conformità alla Proposta e per verificare ex post eventuali malfunzionamenti, incidenti o usi impropri del sistema;
- **informare le persone fisiche** **che sono soggette all'uso di un sistema di AI** nel caso vengano messi a disposizione sistemi ad alto rischio che prendono decisioni o contribuiscono a prendere decisioni relative a queste ultime.

Il requisito della trasparenza era già emerso in passato all’interno delle Ethics Guidelines for a Trustworthy AI* dell’High-Level Expert Group on Artificial Intelligence, nelle raccomandazioni dell’OECD e nelle linee guida redatte dalla stessa Microsoft nel 2018 quando si era parlato di indicare delle prime linee guida per lo sviluppo di AI. Come sopra evidenziato, all’interno di questa Proposta di Regolamento il rispetto del principio di trasparenza viene ripreso più volte sotto diversi aspetti. Tale analisi non soltanto ne sottolinea l’importanza ma evidenzia anche la sempre necessaria conformità con la normativa europea in tema di tutela del dato personale. Il Regolamento Generale sulla Protezione dei Dati (Reg. UE 2016/679), infatti, al fine di garantire la tutela dei diritti e delle libertà fondamentali delle persone fisiche in relazione al trattamento dei dati personali prevede che il trattamento dei dati personali debba essere effettuato in modo trasparente nei confronti degli interessati, che devono essere informati in modo completo, chiaro e conciso circa l'elaborazione dei loro dati personali.
### <a name="_toc138674436"></a>**4.4 Obbligo di supervisione**
**Fornitori**

La Proposta di Regolamento stabilisce che, per i sistemi ad alto rischio, è necessaria una **supervisione umana** che deve essere:

- proporzionata ai rischi del sistema;
- effettuata attraverso soggetti preposti al controllo che (i) conoscono le capacità e i limiti dall’AI, (ii) siano in grado di monitorarne il funzionamento, (iii) siano capaci di intervenire o interrompere il sistema se necessario. 

L’obbligo di supervisione non è previsto solamente da un punto di vista generale, come appena affrontato, ma prevede **un ulteriore controllo per quei sistemi di AI ad alto rischio che prendono decisioni** attraverso una verifica e una conferma da almeno due persone fisiche competenti. I soggetti designati alla sorveglianza devono: 

- avere conoscenze sufficienti nell’ambito di questa tecnologia;
- avere l'autorità necessaria per esercitare tale funzione;
- essere specificamente informate del rischio di automazione o di bias di conferma.

**Utenti**

Anche gli Utenti hanno l’obbligo di sovrintendere ed agevolare la supervisione attraverso: 

- adeguati strumenti che permettano agli esseri umani di vigilare sul corretto funzionamento del sistema;
- l’individuazione di supervisori competenti, qualificati e che dispongano delle risorse necessarie per un’effettiva supervisione;
- l’adozione di misure tecniche e organizzative adeguate. 

Più nello specifico nella categoria degli Utenti all’interno della Proposta viene individuata la figura degli **Importatori** che devono:

- garantire la conformità dell’AI ad alto rischio prima di immetterla sul mercato;
- astenersi dall’immettere l’AI sul mercato qualora dovessero sorgere dubbi in relazione alla conformità del sistema.

Altra categoria specifica individuabile all’interno degli Utenti è quella dei **Distributori** ai quali viene fatto obbligo di:

- verificare la conformità dell’AI prima di immetterla sul mercato;
- astenersi dall'emettere l'AI sul mercato qualora dovessero sorgere dubbi in relazione alla conformità del sistema;
- garantire che, mentre un sistema di AI ad alto rischio è sotto la loro responsabilità, le condizioni di immagazzinamento o di trasporto non mettano a rischio la sua conformità. 

La mancata supervisione umana, soprattutto in ambito decisione è stata già rilevata in passato come elemento critico nell’utilizzo di AI. Nel 2018, Amazon ha dovuto interrompere l’utilizzo di un sistema di assunzione basato su algoritmi essendo stato rilevato come questo producesse risultati discriminatori nei confronti delle donne selezionando solamente curricula maschili. Il sistema era stato addestrato su una grande quantità di curriculum vitae inviati dagli aspiranti dipendenti negli anni precedenti che erano a maggioranza uomini, questo ha erroneamente fatto ritenere alla macchina che il curriculum di un uomo fosse preferibile rispetto a quello di una donna effettuando così una selezioni a priori. Un altro caso più recente ha, invece, riguardato l’utilizzo di AI da parte delle forze dell’ordine e come l’erronea decisione di quest’ultima abbia inciso su uno dei diritti fondamentali dell’uomo: la libertà. Nel 2019 il dipartimento di polizia di Detroit ha arrestato un uomo basandosi sull’analisi dei filmati fatta da una tecnologia di riconoscimento facciale che si è basata sulla foto della patente di guida. Il soggetto è stato trattenuto in custodia per 30 ore, prima di essere rilasciato avendo gli agenti di polizia riconosciuto l'errore. L'episodio è stato riportato dall'American Civil Liberties Union (ALCU), che ha presentato una denuncia contro il Dipartimento di Polizia di Detroit per violazione dei diritti civili. L'ACLU ha criticato l'uso dell'algoritmo di riconoscimento facciale come fonte unica di prove, poiché è stato dimostrato che il sistema può produrre risultati errati e discriminatori. L’obbligo di supervisione umana rappresenta, dunque, un importante meccanismo di controllo per garantire che le decisioni prese da sistemi di AI che possono avere un impatto significativo sulla vita delle persone siano giuste, etiche e conformi alle leggi e ai regolamenti applicabili. Si tratta di sistemi che possono imparare e adattarsi in modo autonomo, il che significa che la loro attività potrebbe diventare imprevedibile o incontrollabile senza una supervisione adeguata.
### <a name="_toc138674437"></a>**4.5 Obblighi di conformità ai requisiti europei**
**Fornitori**

La Proposta dedica anche alcuni articoli all'individuazione di obblighi che potremmo inquadrare come obblighi di conformità ai requisiti europei. Viene richiesto infatti che il sistema di AI ad alto rischio: 

- sia sottoposto ad una procedura di valutazione della sua conformità, prima dell'immissione sul mercato o della messa in servizio;
- sia accompagnato da una dichiarazione di conformità;
- presenti la marcatura CE;
- sia conforme ai requisiti di accessibilità;
- venga registrato all’interno della banca dati dell’Unione;
- preveda la nomina di un rappresentante nel territorio dell’Unione se il Fornitore non ha sede all’interno dell’UE.
### <a name="_toc138674438"></a>**4.6 Obblighi di interazione e collaborazione con le Autorità**
**Fornitori**

I Fornitori devono conservare per un periodo di 10 anni dall'immissione sul mercato o dalla messa in servizio del sistema di AI: 

- la documentazione tecnica;
- la documentazione relativa al sistema di gestione della qualità, ove applicabile;
- la documentazione relativa alle modifiche approvate dagli organismi notificati;
- le decisioni e gli altri documenti rilasciati dagli organismi notificati ed infine;
- la dichiarazione di conformità UE.

Viene infatti previsto che l'Autorità nazionale competente, con richiesta motivata, possa:

- domandare ai Fornitori/Installatori di AI di dimostrarne la conformità della tecnologia fornendo informazioni e documentazione della compliance agli obblighi individuati dalla Proposta di Regolamento;
- richiedere l'accesso ai log generati. 

L’Autorità non può divulgare i dati commerciali ottenuti e ha l’obbligo di trattarli con riservatezza. Le disposizioni in merito gli obblighi di interazione e collaborazione con le Autorità non si esauriscono con la messa a disposizione della documentazione, ma viene anche previsto che qualora il Fornitore rilevi:

- eventuali non conformità;
- un rischio che può potenzialmente influire negativamente su determinati aspetti della vita delle persone come la salute la sicurezza, i diritti fondamentali e la democrazia;
- un incidente grave; 

questi debba riportarli immediatamente alle Autorità di vigilanza nazionali, ai Distributori, agli Importatori ed agli Installatori e devono essere adottate misure correttive. In caso di incidenti gravi il termine per la comunicazione alle Autorità è di 72 ore.

**Utenti**

Gli Utenti compresi Fornitori, Importatori e Distributori, hanno sempre l'obbligo di: 

- collaborare con le Autorità nazionali competenti;
- informare le Autorità, il Fornitore, l’Importatore o il Distributori quando ritengono che l’uso dell’AI presenti un rischio a livello nazionale;
- informare le Autorità, il Fornitore, l’Importatore o il Distributori quando individuano un incidente grave o un malfunzionamento.

La normativa mira a garantire che i sistemi di AI ad alto rischio siano utilizzati in modo sicuro e che eventuali problemi siano individuati il prima possibile, per questo prevede una serie di obblighi di collaborazione con l’Autorità. Queste disposizioni sono atte ad accertare che i sistemi di AI ad alto rischio siano conformi ai requisiti di regolamentazione dell'UE e vengano utilizzati correttamente. 
### <a name="_toc138674439"></a>**4.7 Obblighi di accuratezza, robustezza, cybersicurezza e resilienza**
**Fornitori** 

L’accuratezza, robustezza, cybersicurezza e resilienza sono caratteristiche di estrema rilevanza in tema di nuove tecnologie e restano centrali anche all’interno della Proposta di regolamento in tema di AI. Per questa ragione è previsto che i Fornitori debbano **istituire un sistema di monitoraggio** post-vendita che: 

- raccolga, documenti e analizzi attivamente i dati pertinenti per tutta la durata di vita del prodotto;
- sia integrato nella documentazione tecnica;
- sia proporzionato alla natura delle tecnologie e ai rischi del sistema. 

L’obiettivo è permettere ai Fornitori di valutare la conformità dei sistemi di AI ai requisiti previsti dalla normativa. 

**Utenti**

Come molti degli obblighi finora visti, anche quello di accuratezza, robustezza, cibersicurezza e resilienza non riguarda esclusivamente i Fornitori ma anche gli Utenti. È infatti previsto che questi:

- garantiscano il monitoraggio adeguato;
- garantiscano l’aggiornamento delle misure di robustezza e cybersecurity. 

Di seguito una tabella riassuntiva degli obblighi dei fornitori e degli utenti nei sistemi ad alto rischio:

<table><tr><th colspan="4" valign="top">ALTO RISCHIO</th></tr>
<tr><td colspan="2" valign="top">FORNITORI</td><td colspan="2" valign="top">UTENTI</td></tr>
<tr><td colspan="1" valign="top">Art.</td><td colspan="1">Obblighi</td><td colspan="1" valign="top">Art.</td><td colspan="1">Obblighi</td></tr>
<tr><td colspan="1" rowspan="4">9</td><td colspan="1" valign="top">(2)(a) identificazione e valutazione dei rischi sui diritti fondamentali delle persone.</td><td colspan="1" rowspan="3">26</td><td colspan="1">(1) per l’importatore verifica che il fornitore abbia eseguito l'appropriata procedura di valutazione della conformità, redatto la documentazione tecnica, nominato rappresentante autorizzato nel territorio dell’Unione.</td></tr>
<tr><td colspan="1" valign="top">(2)(c) valutazione dei rischi significativi emergenti derivanti dall'analisi dei dati raccolti dal sistema di monitoraggio successivo all'immissione sul mercato.</td><td colspan="1">(3) importatori indicano il proprio nome, la denominazione commerciale registrata o il marchio registrato e l'indirizzo al quale possono essere contattati sul sistema di IA e sull'imballaggio o sulla documentazione di accompagnamento.</td></tr>
<tr><td colspan="1" valign="top">(2)(d) utilizzo di misure adeguate e mirate di gestione dei rischi.</td><td colspan="1">(5) importatori forniscono alle Autorità nazionali competenti, su richiesta motivata, tutte le informazioni e la documentazione necessarie a dimostrare la conformità di un sistema e l'accesso ai log nella misura in cui tali log sono sotto il controllo del fornitore.</td></tr>
<tr><td colspan="1" valign="top">(5) sottoporre AI ad una prova al fine di individuare le misure di gestione dei rischi più appropriate e mirate e soppesare tali misure rispetto ai potenziali benefici e agli obiettivi del sistema.</td><td colspan="1" rowspan="2">27</td><td colspan="1" valign="top">(1)  i distributori verificano che il sistema di IA ad alto rischio rechi la necessaria marcatura di conformità CE, che sia accompagnato dalla documentazione e dalle istruzioni per l'uso richieste e che il fornitore e l'importatore del sistema, a seconda dei casi, abbiano rispettato gli obblighi previsti dal presente regolamento.</td></tr>
<tr><td colspan="1">10</td><td colspan="1" valign="top">(1) sviluppo sulla base di set di dati di addestramento, convalida e prova che soddisfano determinati criteri di qualità (art 10) tenendo in considerazione la fattibilità in base allo specifico segmento di mercato o all'ambito di applicazione.</td><td colspan="1" valign="top">(5) su richiesta motivata i distributori forniscono alla Autorità tutte le informazioni e la documentazione in loro possesso o a loro disposizione, necessarie per dimostrare la conformità di un sistema.</td></tr>
<tr><td colspan="1">11</td><td colspan="1" valign="top">(1) redazione ed aggiornamento della documentazione tecnica, che dimostra la conformità dal sistema ai requisiti, prima dell'immissione sul mercato o della messa in servizio di tale sistema.</td><td colspan="1" rowspan="5">29</td><td colspan="1" valign="top">l'utente garantisce che tali dati di input siano pertinenti alla luce della finalità prevista del sistema di IA ad alto rischio</td></tr>
<tr><td colspan="1" rowspan="2">12</td><td colspan="1" valign="top">(1) capacità di registrazione automatica di log durante il funzionamento. Tali capacità di registrazione devono essere conformi allo stato dell'arte e alle norme riconosciute o alle specifiche comuni.</td><td colspan="1" valign="top">gli utenti devono monitorare il funzionamento del sistema di IA ad alto rischio sulla base delle istruzioni per l'uso.</td></tr>
<tr><td colspan="1" valign="top">(2a) registrazione del consumo di energia, di misurazione o calcolo dell'uso delle risorse e l'impatto ambientale del sistema durante tutte le fasi del ciclo di vita del sistema.</td><td colspan="1" valign="top">gli utenti informano il fornitore o il distributore e sospendono l'uso del sistema se rilevano un rischio o individuano un incidente grave o un malfunzionamento</td></tr>
<tr><td colspan="1">13</td><td colspan="1" valign="top">(3) istruzioni d'uso comprensibili che includono informazioni concise, corrette, chiare e, per quanto possibile, complete e siano ragionevolmente pertinenti, accessibili e comprensibili per gli utenti.</td><td colspan="1" valign="top">gli utenti conservano i log generati automaticamente dai loro sistemi di IA ad alto rischio, nella misura in cui tali log sono sotto il loro controllo.</td></tr>
<tr><td colspan="1">14</td><td colspan="1" valign="top">(1) progettazione che garantisca una sorveglianza efficace da parte delle persone fisiche.</td><td colspan="1" valign="top">utenti di sistemi di IA ad alto rischio usano le informazioni fornite a norma dell'articolo 13 per adempiere al loro obbligo di effettuare una valutazione d'impatto sulla protezione dei dati a norma dell'articolo 35</td></tr>
<tr><td colspan="1">15</td><td colspan="1" valign="top">(1) raggiungere un livello appropriato di accuratezza, robustezza, sicurezza e cybersicurezza e funzionare in modo coerente sotto questi aspetti per tutto il loro ciclo di vita. La conformità a questi requisiti deve comprendere l'implementazione di misure all'avanguardia, in base allo specifico segmento di mercato o ambito di applicazione.</td><td colspan="1">29a</td><td colspan="1" valign="top">i responsabili della distribuzione (obbligo del distributore) effettuano una valutazione dell'impatto del sistema nel contesto specifico di utilizzo.</td></tr>
<tr><td colspan="1" rowspan="8">16</td><td colspan="1" valign="top">(aa) indicare il proprio nome, la denominazione commerciale registrata o il marchio registrato, nonché il proprio indirizzo e le informazioni di contatto sul sistema di IA o sulla documentazione di accompagnamento.</td><td colspan="1"></td><td colspan="1" valign="top"></td></tr>
<tr><td colspan="1" valign="top">(c) redigere e conservare la documentazione tecnica (art.11).</td><td colspan="1" valign="top"></td><td colspan="1" valign="top"></td></tr>
<tr><td colspan="1" valign="top">(d) conservazione dei log (art.20).</td><td colspan="1" valign="top"></td><td colspan="1" valign="top"></td></tr>
<tr><td colspan="1" valign="top">(e) sottoporre a procedura di valutazione l’AI prima dell’immissione sul mercato (art.43).</td><td colspan="1" valign="top"></td><td colspan="1" valign="top"></td></tr>
<tr><td colspan="1" valign="top">(ea) redigere dichiarazione di conformità UE (art.48).</td><td colspan="1" valign="top"></td><td colspan="1" valign="top"></td></tr>
<tr><td colspan="1" valign="top">(eb) apposizione della marcatura CE (art.49).</td><td colspan="1" valign="top"></td><td colspan="1" valign="top"></td></tr>
<tr><td colspan="1" valign="top">(j) Su richiesta motivata di un'Autorità nazionale di vigilanza, dimostrare la conformità del sistema.</td><td colspan="1" valign="top"></td><td colspan="1" valign="top"></td></tr>
<tr><td colspan="1" valign="top">(k) Assicurare che il sistema di IA ad alto rischio sia conforme ai requisiti di accessibilità.</td><td colspan="1" valign="top"></td><td colspan="1" valign="top"></td></tr>
<tr><td colspan="1">17</td><td colspan="1" valign="top">(1) redazione di politiche procedure ed istruzioni.</td><td colspan="1" valign="top"></td><td colspan="1" valign="top"></td></tr>
<tr><td colspan="1">21</td><td colspan="1" valign="top">(1) I fornitori devono adottare immediatamente le misure correttive necessarie per rendere conforme il dispositivo, ritirarlo o richiamarlo qualora ritengano che non sia conforme alla normativa.</td><td colspan="1" valign="top"></td><td colspan="1" valign="top"></td></tr>
<tr><td colspan="1">22</td><td colspan="1" valign="top">(1) Informare immediatamente le Autorità nazionali competenti degli Stati membri in merito a non conformità rilevate</td><td colspan="1" valign="top"></td><td colspan="1" valign="top"></td></tr>
<tr><td colspan="1">23</td><td colspan="1" valign="top">(1) fornire a tale Autorità tutte le informazioni e la documentazione necessarie per dimostrare la conformità del sistema. Su richiesta motivata di un'Autorità nazionale competente, forniscono l'accesso ai log.</td><td colspan="1" valign="top"></td><td colspan="1" valign="top"></td></tr>
<tr><td colspan="1">25</td><td colspan="1" valign="top">nomina eventuale di un rappresentante nel territorio dell’Unione.</td><td colspan="1" valign="top"></td><td colspan="1" valign="top"></td></tr>
<tr><td colspan="1">50</td><td colspan="1" valign="top">(1) conservazione per 10 anni della documentazione tecnica, delle procedure ed istruzioni, della documentazione relativa alle modifiche, della documentazione relativa alle modifiche e la dichiarazione di conformità UE.</td><td colspan="1" valign="top"></td><td colspan="1" valign="top"></td></tr>
<tr><td colspan="1">51</td><td colspan="1" valign="top">(1) prima di immettere sul mercato o mettere in servizio un sistema di IA ad alto rischio il fornitore o, se del caso, il rappresentante autorizzato registra tale sistema nella banca dati dell'UE.</td><td colspan="1" valign="top"></td><td colspan="1" valign="top"></td></tr>
<tr><td colspan="1">61</td><td colspan="1" valign="top">(1) sistema di monitoraggio successivo all'immissione sul mercato.</td><td colspan="1" valign="top"></td><td colspan="1" valign="top"></td></tr>
<tr><td colspan="1">62</td><td colspan="1" valign="top">(1) i fornitori segnalano all'Autorità nazionale di vigilanza degli Stati membri in cui si è verificato l'incidente o la violazione qualsiasi incidente grave relativo a tali sistemi che costituisca una violazione degli obblighi previsti dal diritto dell'Unione a tutela dei diritti fondamentali.</td><td colspan="1" valign="top"></td><td colspan="1" valign="top"></td></tr>
</table>

## **5. <a name="_toc138674440"></a>AI e contenuti creativi: una breve introduzione**
L'AI ebbe inizio con il celebre paper di Alan Turing intitolato "Computer Machinery and Intelligence", nel quale si chiedeva se le macchine potessero pensare. Il paper si basava sull'assunto che l'intelligenza fosse equivalente alla logica. Tuttavia, nel corso degli anni, si è evoluto il concetto di intelligenza, includendo tratti come l'apprendimento dall'esperienza e la comprensione del linguaggio naturale.

Il mondo dell'Intelligenza Artificiale (AI) ha fatto passi da gigante negli ultimi decenni, evolvendo rapidamente fino ad essere una parte essenziale nella vita di tutti i giorni. Il Machine Learning (ML) e il Natural Language Processing (NLP) sono, infatti, due sottoinsiemi dell'AI che hanno rivoluzionato il modo in cui le macchine interagiscono e comunicano con gli esseri umani.

Sviluppatori informatici ed artisti sono impegnati da tempo nello sviluppo di soluzioni innovative in grado di destrutturare il concetto stesso di arte e creazione.

In passato, la maggior parte delle opere generate al computer si basava fortemente sull’input creativo del programmatore: la macchina poteva considerarsi uno strumento o un attrezzo, in fin dei conti non diverso da un pennello o da una tela. 

Negli ultimi anni abbiamo assistito alla rivoluzione tecnologica che ci ha condotto ad un ripensamento del rapporto d’interazione tra umani, macchine e processi creativi, anche grazie alle nascenti soluzioni che hanno permesso ai software di comprendere il linguaggio naturale tanto da consentire a tutti (non solo ai programmatori) di stabilire una via di comunicazione diretta.  

Questa rivoluzione è stata sostenuta dal rapido sviluppo di software di apprendimento automatico: un sottoinsieme di intelligenza artificiale che produce sistemi autonomi in grado di apprendere e “creare”, senza essere specificamente programmati da un essere umano. Una caratteristica di rilievo di questo tipo di intelligenza artificiale generativa è che, mentre i programmatori o gli utilizzatori possono impostare o suggerire i parametri, il lavoro viene poi, appunto, generato dal programma stesso in un processo simile al pensiero umano.

Tornando all’arte generativa, un recente esempio di commistione tra arte e intelligenza artificiale è stato il caso del ritratto di “Edmond Belamy”. Si tratta di una tela creata grazie a un’intelligenza artificiale, più precisamente un GAN (Generative Adversarial Network), sviluppato dal collettivo francese Obvious[^1], che ha come scopo quello di “*democratizzare l’AI attraverso l’arte*”, e dal significativo contributo di un giovane programmatore, Robbie Barrat, che aveva condiviso i suoi algoritmi online con una licenza open-source, dai quali avevano attinto i fondatori del collettivo per le loro creazioni. L’opera raffigura il melanconico Edmond ed è parte di una serie di tele, denominata “La Famille de Belamy”, una famiglia fittizia il cui titolo ricalca in francese il cognome del ricercatore Ian Goodfellow (*ndr* “bel ami” in francese significa “buon amico”, ossia “good fellow”), niente di meno che colui che ha sviluppato, nel 2014, l’algoritmo alla base del GAN. L’opera è stata battuta all’asta nell’ottobre del 2018 presso la sede newyorchese di Christie’s la quale l’ha definita “*l’arrivo dell’arte AI sul palcoscenico mondiale delle aste*”[^2].

La parte più interessante della vicenda è stata senza dubbio il successivo dibattito, infatti, in seguito al comunicato di Christie’s hanno sollevato arcane domande: poteva quell’opera essere considerata autentica? E chi era effettivamente l’autore: Obvious? L’AI che l’aveva generata, o Barrat, che aveva sviluppato l’algoritmo? E perché non lo stesso Goodfellow, che aveva creato originariamente il GAM? 

Per citare ulteriori casi che hanno suscitato non solo vecchie, ma anche nuove e più cocenti criticità sull’utilizzo dell’AI per realizzare contenuti creativi, troviamo il caso del brano musicale “Called Heart On My Sleeve”, diventato virale sui social network nell’aprile del 2023. 

La peculiarità del brano sta nel fatto che le voci degli interpreti apparentemente sono quelle dei popolari cantanti americani Drake e The Weeknd, ma i due non hanno mai prestato il proprio contributo – né il consenso – alla realizzazione del brano. Si tratta, infatti, di un “fake”, realizzato da un creator digitale noto come @Ghostwriter attraverso un software di AI generativo opportunamente allenato. Il brano è stato condiviso sulle maggiori piattaforme di streaming musicale, dalle quali è stato poi tempestivamente rimosso, pur raggiungendo centinaia di migliaia di ascolti, in seguito alle segnalazioni dei titolari dei diritti violati.

C’è chi, ancora, ha utilizzato tali sistemi di AI generativa per riesumare vecchie glorie della musica, come il leggendario Kurt Cobain, scomparso nel 1994, ed oggi apparentemente interprete di *cover* mai realizzate, condivise, tuttavia, da migliaia di utenti in rete. 

Fino a che punto, quindi, questo tipo di sperimentazione creativa è tollerabile, e quando invece ciò inizia a pregiudicare in maniera rilevante i diritti e gli interessi degli individui che vedono il frutto del proprio lavoro creativo, se non la loro stessa voce o immagine, utilizzato da sistemi di intelligenza artificiale – e dai loro sviluppatori – per creare nuove fonti di profitto?

Provare a rispondere a questo interrogativo è fondamentale per comprendere a chi appartiene  l’opera generata da un software di intelligenza artificiale e quali sono i rischi nell’utilizzo di quest’ultima.
## **6. <a name="_toc138674441"></a>I rischi connessi all’utilizzo di un’opera generata da intelligenza artificiale** 
Quali sono i possibili rischi sotto il profilo del diritto d’autore a cui va incontro chi genera un prodotto sia esso un immagine, un video, un brano musicale, un testo, mediante un software di intelligenza artificiale? 

Per rispondere a questa domanda ci sono alcune rilevanti questioni giuridiche da approfondire: 

- Il prodotto di un sistema di AI generativa può essere protetto dal Diritto d’autore? 
- Quali sono i soggetti autorizzati a sfruttare i prodotti generati da AI?
- È possibile che il prodotto generato mediante un software di AI violi i diritti di proprietà di terzi? 
### <a name="_toc138674442"></a>**6.1 Il prodotto di un sistema di AI generativa può essere protetto dal diritto d’autore?**
In passato, come riportato anche dall’autoritaria posizione del WIPO[^3] (World Intellectual Property Organization), la questione relativa ai diritti sulle opere d’arte generate attraverso una macchina non veniva messa in discussione. 

Da un punto di vista soggettivo, si è sempre rilevata la necessità del contributo umano ai fini della creazione di un’opera d’arte protetta. Da un punto di vista oggettivo, gli interpreti hanno sempre ritenuto che prima della macchina ci sia un “creatore”, un *deus ex machina* che ha sviluppato algoritmi e che ha immesso immagini ed informazioni. Corollario di questo orientamento è che la macchina viene considerata quale un mero strumento a supporto del processo creativo dell’autore, esclusivo beneficiario, pertanto, dei diritti morali e patrimoniali sull’opera creativa, ma pure delle eventuali responsabilità. 

In base alla normativa italiana sul diritto d’autore, in particolare secondo l’articolo 6 della LdA[^4] dispone che il titolo originario dell’acquisto del diritto è costituito dalla creazione dell’opera. In altri termini, la creazione dell’opera è *condicio* sufficiente per vedersi riconosciuti i diritti previsti dalla Legge, senza che l’autore debba compiere ulteriori formalità e registrazioni, come invece è richiesto per la tutela dei titoli di proprietà industriale, come il brevetto o i marchi. 

Le opere d’ingegno possono peraltro essere protette dal Diritto d’autore solo se originali e creative[^5], e la maggior parte delle definizioni di originalità tradizionalmente individua un autore umano. 

L’art 8 della LdA prevede che “è reputato autore dell'opera, salvo prova contraria, chi è in essa indicato come tale, nelle forme d'uso, ovvero è annunciato come tale”, e che al medesimo sono riconosciuti due categorie di diritti: i diritti morali, perpetui e indisponibili, a tutela della personalità e della reputazione dell’autore; e i diritti di utilizzazione economica, ossia facoltà esclusive di sfruttamento dell’opera, tra loro indipendenti e trasferibili, suscettibili di procurare degli utili al titolare, che durano “tutta la vita dell'autore e sino al termine del settantesimo anno solare dopo la sua morte”[^6]. Il riferimento a personalità dell’autore da una parte, e morte dell’autore dall’altra, esprimono la tradizionale concezione per cui l’autore dell’opera debba essere un essere umano.

La giurisprudenza italiana[^7] ha contemplato anche l’ipotesi del “*work for hire*”, rilevando la necessità che il *work for hire* trovi fondamento in un contratto d’opera. Solo in questo caso il committente acquista il diritto di sfruttamento economico dell’opera in modo automatico ed originario, senza necessità di un ulteriore contratto di cessione, che ai sensi dell’art. 110 LdA deve provarsi necessariamente per iscritto. Va da sé che un contratto d’opera sottende implicitamente un rapporto tra persone fisiche, per cui risulta complesso inquadrare il processo creativo sviluppato tramite una AI nell’ambito del *work for hire.*

Negli Stati Uniti, il Copyright Office, l’ente governativo ufficiale che si occupa della registrazione delle opere all’interno di un apposito catalogo pubblico, da sempre dichiara di provvedere alla registrazione di* “*un'opera originale d'autore, a condizione che l'opera sia stata creata da un essere umano*”. Questa posizione deriva da risalente giurisprudenza delle Corti americane[^8] che specifica che la legge sul *copyright* protegge solo “*i frutti del lavoro intellettuale*” che “*si fondano sulle capacità creative della mente*”. 

Analogamente, nell’ambito di un procedimento giudiziario instauratosi presso le Corti australiane[^9], è stato dichiarato che un’opera generata con l’intervento di un computer non può essere protetta dal diritto d’autore, perché non è prodotta da un essere umano.

In Europa, la Corte di Giustizia ha dichiarato in varie occasioni, in particolare nella storica decisione “Infopaq”[^10], che il diritto d’autore si applica solo alle opere originali, e che l’originalità deve riflettere la “*creazione intellettuale dell’autore*”. Da ciò ne deriva che l’opera protetta richiede la creazione di un autore umano.

L’attribuzione della paternità dell’opera al programmatore di una AI è codificata in alcuni Paesi come Hong Kong (SAR), India, Irlanda, e Regno Unito*.* 

La sez. 9(3) del Copyright, Designs and Patents Act (CDPA) inglese dispone, ad esempio, che “nel caso di un'opera letteraria, drammatica, musicale o artistica generata al computer, si considera autore la persona da cui sono state prese le disposizioni necessarie per la creazione dell'opera”. La sez. 178 dello stesso CDPA definisce un’opera generata al computer come un’opera “generata al computer in circostanze tali da non avere un autore umano dell'opera”. La ratio evidente della norma è quella di individuare un’eccezione alla necessità che l’opera abbia paternità umana, riconoscendo quindi allo sviluppatore dell’AI l’attività svolta per la creazione di un programma in grado di generare opere, ed attribuendogli di conseguenza la paternità delle stesse.

La soluzione lascia comunque aperte una serie di problematiche. La Legge dovrebbe riconoscere il contributo del programmatore, o dell’utente del programma? La giurisprudenza britannica, apparentemente cauta sul tema, sembra indicare che la questione potrebbe essere risolta caso per caso.[^11]

Un caso che ha destato interesse oltreoceano, il cui eco è giunto anche alle orecchie dei giuristi nazionali attenti alle problematiche dell’AI nel mondo dell’arte, è la vicenda tra il dottor Stephen Thaler e lo United States Copyright Office.

Il dottor Thaler è un pioniere nel campo dell’intelligenza artificiale, nonché inventore del paradigma della “Creativity Machine(R)”, un sistema neurale artificiale che viene perturbato dal rumore, in modo da stimolare la generazione di nuove idee e strategie ed attivo in quasi tutte le discipline umane, contribuendo a scienza, tecnologia, arte, musica. 

Per meglio chiarire, lo US Copyright Office assolve a funzioni che all’interno del nostro ordinamento vengono esercitate in parte dall’Ufficio Italiano dei Brevetti e Marchi (“UIBM”), che si occupa della registrazione dei titoli di proprietà industriale sui marchi e brevetti, ai fini della loro validità territoriale, ed in parte dal Ministero della cultura e dalla Società Italiane Autori ed Editori (spesso abbreviata in “SIAE”) ai quali, ai sensi dell’art. 103 LDA, è affidata rispettivamente la gestione di un registro pubblico generale delle opere protette ai sensi della LDA, ed di un registro pubblico speciale per le opere cinematografiche e le opere audiovisive. Il 3 novembre 2018, il dottor Thaler presentava quindi una domanda di registrazione di un’opera grafica intitolata “*A Recent Entrance to Paradise*” presso lo US Copyright Office, affinché la creazione venisse inserita nel pubblico registro delle opere protette. 

La peculiarità della vicenda, è che all’interno della richiesta l’autore veniva identificato come “Creativity Machine”, mentre il dottor Thaler, quale mero richiedente, allegava le seguenti specifiche: “*proprietà della macchina*”; “*è stata creata autonomamente da un algoritmo di computer in esecuzione su una macchina*”, rilevando altresì che l’opera fosse stata realizzata dalla macchina come *work for hire* in favore dello stesso Thaler. 

In una lettera del 12 agosto 2019, uno specialista del Copyright Office rigettava la richiesta, ritenendo che mancasse* “*la paternità umana necessaria a sostenere un diritto d'autore*”.

In seguito al primo rifiuto, il dottor Thaler richiedeva all'Ufficio di riconsiderare la propria posizione, sostenendo, con argomentazioni per lo più politiche, che il requisito della paternità umana fosse incostituzionale. 

Dopo aver riesaminato l’opera alla luce dei punti sollevati e rivalutato le rivendicazioni del richiedente, l’Ufficio concludeva nuovamente con un rigetto, rilevando ancora la mancanza della necessaria paternità umana, in quanto il dottor Thaler non avrebbe fornito “*alcuna prova di un sufficiente apporto creativo o dell’intervento di un autore umano nell’opera*”. L’Ufficio, in questo secondo rifiuto, chiosava altresì di non voler abbandonare l'interpretazione da lungo tempo seguita dalla Corte Suprema e dei precedenti giudiziari delle corti inferiori, secondo la quale “*un’opera soddisfa i requisiti legali e formali della protezione del copyright solo se è creata da un autore umano*”, e rilevava di non essere a conoscenza di diversi orientamenti nelle corti statunitensi, tali da riconoscere diritti d’autore alle macchine.

Nel marzo del 2022, la Commissione di revisione dell’US Copyright Office dopo aver confermato il rifiuto di riconoscere la paternità di un’opera, ed i relativi diritti, ad una macchina, ha pubblicato delle linee guida ufficiali[^12], per fornire indicazioni relativamente alla registrazione di opere che siano state realizzate con il contributo di un sistema di AI.

L’Ufficio specifica come l’agenzia preposta al sistema di registrazione sia aperta a compiere verifiche su opere realizzate da autorialità umana unita a materiale generato dalla tecnologia. In presenza di opere generate in tal modo, l’Ufficio considererà e valuterà se i contributi dell’AI siano il risultato di una mera “*riproduzione meccanica*”, o dell’”*originale concezione mentale dell’autore, alla quale ha dato forma visibile*”. La risposta dell’Ufficio dipenderà, dunque, dalle circostanze descritte e provate in fase di registrazione, in particolare da come funziona lo strumento di AI e da come è stato utilizzato per creare l’opera finale.  

L’Ufficio fornisce poi alcune indicazioni di massima sulla compilazione della domanda, ma la conclusione che prospetta, di fatto, è che la verifica dovrà essere accuratamente svolta caso per caso. Quando la tecnologia AI risulterà idonea a determinare gli elementi espressivi del suo output, il materiale generato non sarà il prodotto di autorialità umana e, conseguentemente, tale materiale non sarà protetto, né potrà essere accolta la richiesta di registrazione.

### <a name="_toc138674443"></a>**6.2. Quali sono i soggetti autorizzati a sfruttare i prodotti generati da AI?** 
Il metodo più semplice per verificare direttamente a chi appartengono i risultati del processo creativo di una intelligenza artificiale generativa anche gratuitamente in rete è quello di leggere i termini di servizio del software che si è utilizzato.

Un primo esempio è quello della versione beta di “Midjourney”[^13] i cui proprietari si presentano come “un laboratorio di ricerca indipendente che esplora nuovi mezzi di pensiero ed espande i poteri immaginativi della specie umana”.

Si tratta di un programma, fruibile tramite la piattaforma Discord[^14], in grado di realizzare opere di illustrazione a partire da semplici input, sotto forma di frasi e suggerimenti dell’utente.	

[I Termini di servizio della piattaforma](https://docs.midjourney.com/docs/terms-of-service) offrono spunti interessanti in merito alle possibili soluzioni negoziali, che legano i titolari della piattaforma agli utenti, relativamente al tema della titolarità di un asset realizzato da un’AI generativa. 

Se l’utente è un membro non pagante della community, e dunque utilizza la versione gratuita, i Termini di servizio prevedono che il medesimo non detiene alcuna proprietà degli asset realizzati tramite il software di AI, bensì una licenza basata sul modello internazionale [Creative Commons Attribution Noncommercial 4.0.](https://creativecommons.org/licenses/by-nc/4.0/), la quale, in breve, consente esclusivamente di condividere e modificare l’asset, con l’obbligo di menzionare la paternità di Midjourney, e senza facoltà di utilizzare l’asset per scopi commerciali.

Peraltro, è previsto che gli asset realizzati dall’utente non pagante siano sempre visibili pubblicamente e modificabili dagli altri utenti sulla piattaforma.

Per gli utenti che acquistano un piano “Pro”, è possibile evitare alcune delle utilizzazioni pubbliche sopra menzionate. I Termini del servizio prevedono, infatti, la possibilità di acquistare un piano di abbonamento, che permette agli abbonati di produrre attraverso l’AI di Midjourney illustrazioni delle quali i medesimi diventano esclusivi proprietari e che, in linea di massima, possono dunque utilizzare commercialmente, limitandone altresì la condivisione in community[^15]: il piano “Pro” prevede infatti la possibilità, per gli imprenditori “*with more than $1,000,000 USD a year in gross revenue*”, di acquistare un componente aggiuntivo che consente di creare le opere in “modalità privata”, di modo che queste non siano visibili dagli altri utenti.

Per confrontarci con un’altra realtà altrettanto in auge, abbiamo esaminato i [Termini di servizio di OpenAI](https://openai.com/policies/terms-of-use)[^16], organizzazione il cui *core* è la ricerca sull'intelligenza artificiale, con lo scopo di promuovere e sviluppare un sistema che coadiuvi l’attività umana. 

Proprio OpenAI ha sviluppato, tra le altre, la piattaforma ChatGPT, ossia il sistema di intelligenza artificiale in grado di fornire risposte verbali immediate all’utente, dietro l’inserimento di appositi *prompt*. Ma non solo. Open AI detiene nel proprio portafoglio diversi progetti vincenti, come ad esempio Dall-E, un sistema in grado di creare immagini e arte su indicazione dell’utente. 

I Termini prevedono che l’utente possa fornire input ai sistemi, e ricevere output generati e restituiti dai sistemi stessi sulla base degli input. L'input e l'output, collettivamente definiti "Contenuti", sono ripartiti come segue: 

- l'utente è proprietario di tutti gli Input, nella misura consentita dalla legge applicabile;
- OpenAI cede all'utente tutti i propri diritti, titoli e interessi sugli output. Ciò significa che l'utente può utilizzare i Contenuti per qualsiasi scopo, inclusi scopi commerciali come la vendita o la pubblicazione;
- OpenAI può utilizzare i Contenuti per fornire e mantenere le proprie tecnologie, rispettare la legge applicabile e applicare le nostre politiche. 
### <a name="_toc138674444"></a>**6.3. È possibile che il prodotto generato mediante un software di AI violi i diritti di proprietà di terzi?** 
Uno spunto interessante per rispondere a questo quesito è fornito proprio nei Termini di servizio di OpenAI i quali riconoscono che a causa della natura dell’apprendimento automatico dei propri sistemi, l'output potrebbe non essere unico tra gli utenti e i sistemi potrebbero generare lo stesso output o un output simile per OpenAI o per una terza parte. La stessa società propone il seguente esempio: “*l’utente può fornire input a un modello come "Di che colore è il cielo?" e ricevere output come "Il cielo è blu". Anche altri utenti possono porre domande simili e ricevere la stessa risposta. Le risposte richieste e generate da altri utenti non sono considerate Contenuti dell'utente.”* 

Secondo i ToS, le risposte richieste e generate da altri utenti non sono considerate Contenuti dell’utente. La tematica è interessante, in quanto se da una parte riconosce la possibilità che più utenti ottengano lo stesso output, dall’altra non risolve il problema dell’eventuale plagio, che potrebbe sorgere tra i due utenti.

I rischio di plagio, tuttavia, non riguarda solo altre opere prodotte mediante il medesimo software di intelligenza artificiale, ma si estende potenzialmente anche alle opere che sono state utilizzate per l’addestramento e la formazione del dataset del software.  

Per comprendere la questione occorre analizzare, infatti, quali sono le attività necessarie per costruire un sistema di intelligenza artificiale. Con la crescente diffusione di sistemi di AI generativi, il problema è quello di definire a monte le modalità, gli obblighi ed i limiti in capo ai fornitori relativamente al reperimento di dati e materiale per l’addestramento dei propri sistemi, senza che vengano pregiudicati diritti di terzi.

Le operazioni di addestramento dei sistemi di intelligenza artificiale, infatti, prevedono la necessità di fornire un imponente quantitativo di dati al software. E’ legittimo utilizzare materiale di terzi, ad esempio i contenuti di una banca dati, per formare l’AI? E cosa accade se il prodotto dell’AI è estremamente simile ad uno o più dei contenuti che sono stati dati in pasto al software per la sua formazioen?

La questione è tutt’altro che teorica, i Tribunali Statunitensi sono stati i primi, all’inizio del 2023, a vedersi chiamati a rispondere a questa domanda a seguito del deposito dei primi procedimenti nei confronti di società titolari di sistemi di intelligenza artificiale generativa.

La class action avviata dalle tre artiste americane Sarah Andersen, Kelly McKernan e Karla Ortiz contro Stability AI Ltd, Midjourney Inc. e DeviantArt Inc., tre società titolari di sistemi di AI generativa per la realizzazione di contenuti creativi, è stato uno dei primi procedimenti in cui è stata valutata la potenziale violazione dei diritti d’autore ad opera delle tecnologie di intelligenza artificiale.

Le artiste hanno, infatti, presentato una* citazione presso il Northen District della California, Divisione di San Francisco, nel gennaio del 2023, affermando la violazione delle leggi sul diritto d’autore contenute nel Copyright Act e nel Digital Millennium Copyright Act (il cd “DMCA”) per due ordini di ragioni:

1. ` `le società avrebbero addestrato le proprie tecnologie su migliaia di opere protette, di fatto attuando delle copie delle medesime, senza il consenso dei legittimi titolari;
1. le società avrebbero permesso che le tecnologie creassero nuove opere sulla base delle precedenti utilizzate per l’addestramento, di fatto creando “opere derivate” senza il consenso dei titolari delle opere originali, che invece è richiesto dalle norme che regolano il diritto d’autore.

Le artiste hanno argomentato, altresì, che se un acquirente, in passato, era interessato a creare una nuova immagine “nello stile” di un determinato artista, egli doveva pagare per commissionare l’opera o per farsi concedere in licenza un’immagine originale da quell’artista. Con le tecnologie delle tre società, invece, gli acquirenti possono utilizzare indebitamente opere dell’artista, in associazione al nome dello stesso, per generare nuove immagini. Secondo le ricorrenti, tale fattispecie inquadrerebbe un’ipotesi di concorrenza sleale attuata dalle società.

Non si sono fatte attendere le difese delle convenute, le quali, nei rispettivi atti, di cui l’ultimo depositato lo scorso aprile 2023, hanno contestato su più fronti le argomentazioni delle ricorrenti.  Ritualmente, ad esempio, è stato eccepito che le ricorrenti non hanno tempestivamente provveduto alla preventiva registrazione presso il Copyright Office americano delle opere asseritamente violate prima di depositare l’atto introduttivo del procedimento, *conditio* apparentemente necessaria ai sensi della Sez. 411(a) del Copyright Act.

Nel merito, ad esempio, è stata opposta l’indeterminatezza delle contestazioni di *copyright infringment* avanzate dalle attrici, rilevando che si dovrebbe valutare caso per caso, per ciascuna opera asseritamente violata, se l’opera successiva costituisca un *derivative work* oppure un *transformative work.*

Proprio il confronto tra “opere derivate” *(derivate works)* ed “opere trasformative” (*transformative work)* è uno spunto interessante che ha animato i commentatori della vicenda. Per meglio comprendere la problematica: si dice che un’opera successiva può rapportarsi a tre livelli con un’opera originaria di cui riprende in qualche modo gli elementi, a seconda del distacco da essa:

1. può discostarsene poco, senza quindi esprimere alcuna creatività autonoma, ed allora non genererà diritti di sorta: si tratterà, in assenza di volontà negoziali diverse, di contraffazione, ossia di mera violazione del diritto patrimoniale di riproduzione dell’opera originaria, oppure, nei casi più estremi, di plagio, intendendosi con ciò un caso di contraffazione in cui l’autore si arroga indebitamente la paternità del lavoro;
1. può discostarsene in misura maggiore, esprimendo l’opera successiva una propria creatività più o meno significativa, ma mantenendo un legame pure significativo con l’opera originaria, in particolare, in termini di riconoscibilità. In questo caso si parla di opera derivata. In Italia, secondo l’art. 4 LdA, ad esempio, le elaborazioni siffatte sono protette, ma “s*enza pregiudizio dei diritti esistenti sull’opera originaria*”. La norma è interpretata nel senso o che l’autore dell’opera originaria ha un diritto di veto sull’utilizzazione dell’opera derivata non autorizzata, oppure, addirittura, nel senso che egli ne diventa contitolare;
1. può discostarsi in misura significativa dall’opera originaria, al punto che quella originaria costituisce “mera ispirazione” di quella successiva. In tale caso, la ripresa di elementi dell’opera originaria è così modesta, che il “debito intellettuale” non è giuridicamente rilevante. In questi casi, nel Diritto statunitense si parla, appunto, di “transformative works”. 

In breve, dunque, un’opera successiva rispetto ad una precedente può costituirne o un plagio/contraffazione, o un’opera derivativa, oppure un’opera “transformativa”, del tutto indipendente dalla prima.

Ma quando l’opera successiva supera la soglia della creatività e può dirsi *transformative work*? 

Esattamente come eccepito dalle compagnie convenute nel procedimento sopra esaminato, il giudizio andrà di volta in volta compiuto confrontando le opere, sulla base del concetto stesso di creatività, semplice o qualificata che sia. Se nell’ambito dell’esame sulla creatività, risulta che il distacco rispetto all’opera originaria è in realtà notevole, al punto da ritenerla mera ispirazione, l’opera successiva sarà autonoma in toto rispetto a quella originaria.

Ma non è tutto. Poco dopo il ricorso di cui sopra, anche l’azienda americana Getty Images, titolare di una vasta banca dati digitale di fotografie fruibili su licenza, ha intentato una causa contro Stability AI Inc presso il Tribunale distrettuale degli Stati Uniti nel Delaware.

Anche in questo caso, la contestazione mossa alla società è quella di aver copiato milioni di immagini dal database di Getty Image, senza alcuna autorizzazione della stessa, né dei titolari delle immagini asseritamente copiate.

Uno spunto interessante che emerge da questa vicenda è che molti commentatori ritengono si possano giustificare condotte come quella di Stability AI Inc sulla base del cd “*fair use*”, in quanto l’attività delle società sviluppatrici di tecnologie AI sarebbe finalizzata ad ottenere un progresso nella ricerca e nella tecnica, ed in quanto lo scopo dei risultati ottenuti – e delle opere così create – sarebbe diverso rispetto a quello del materiale originario utilizzato per addestrare i software. 

La dottrina del *fair use*, in breve, consiste in un sistema generale di eccezioni e limitazioni ai diritti esclusivi riconosciuti ai titolari di un’opera creativa che, entro certi limiti, consente l’utilizzo della stessa senza la necessaria autorizzazione dei titolari.

In base a quanto disposto dalla sez. 107 del Copyright Act, andrebbe considerato lecito e non configgente con le disposizioni in materia di diritto d’autore l’utilizzo dell’opera giustificato da un interesse generale, che viene ritenuto prevalente rispetto a quello personale del titolare. Detto questo, il giudice è chiamata a valutare in concreto l’esistenza del *fair use*, seguendo quattro fattori:

1. lo scopo dell’utilizzo dell’opera, in particolare viene valutato se c’è finalità commerciale oppure uno scopo formativo senza fini di lucro; 
1. la natura dell’opera d’ingegno;
1. la quantità di opera che viene utilizzata, e il ruolo che tale parte ricopre all’interno dell’opera stessa;
1. i potenziali effetti che l’utilizzo di quell’opera tramite *fair use* può causare al mercato dell’opera.

Ad ogni buon conto, le esperienze sopra citate, ancora in fase di discussione innanzi ai Tribunali, permettono di scorgere due macrocategorie di problemi che interessano i sistemi di AI generativa: il primo riguarda l’origine e la titolarità del data set utilizzato per addestrare i sistemi; il secondo riguarda gli output, ossia i risultati prodotti dalla macchina, la titolarità di detti risultati, il loro utilizzo.

L’approccio europeo in materia di eccezioni e limitazioni è invece impostato su una tassativa definizione delle ipotesi specifiche da parte del Legislatore. La conseguenza che ne deriva, è una minor elasticità rispetto al sistema del *fair use* americano.

Con la Direttiva 2019/790, cd “Direttiva Copyright” , la disciplina relativa alle eccezioni e limitazioni – in particolare per l’utilizzo di contenuti delle banche dati – è stata peraltro oggetto di una serie di modifiche ed implementazioni, pensate per favorire sia le attività conservazione del patrimonio culturale, sia la ricerca ed il progresso scientifico.

Gli artt. 3 e 4 della Direttiva Copyright trattano, in particolare, del cd “*text and data mining*”, spesso abbreviato con l’acronimo “TDM”, consistente in un’analisi automatizzata di una grande quantità di informazioni, dati, metadati, immagini, suoni e materiale, in formato digitale, finalizzata ad acquisire nuove conoscenze, rilevare tendenze, modelli, pattern, statistiche. 

Detta attività presuppone, di fatto, una riproduzione del materiale analizzato e, pertanto, può comportare una violazione dei diritti esclusivi dei titolari del materiale riprodotto e dei titolari della banca dati ove il materiale stesso è contenuto. Per tali motivi, la Direttiva ha previsto una serie di eccezioni che consentirebbero la pratica del *text and data mining* sui database, in presenza di determinate condizioni.

A livello nazionale italiano, i citati articoli della Direttiva sono stati trasposti, rispettivamente, agli artt. 70-ter LdA, che riguarda unicamente l’estrazione di materiale per ricerca scientifica e senza finalità di lucro da parte di organismi di ricerca e istituti di tutela del patrimonio culturale, e l’art 70-quater LdA, che consentirebbe invece l’estrazione di materiale presente in reti o in banche dati “*cui si ha legittimamente accesso ai fini dell’estrazione stessa, quando l'utilizzo delle opere e degli altri materiali non è stato  espressamente  riservato dai titolari del diritto d'autore e dei diritti connessi nonché dai titolari delle banche dati”.* A tali condizioni, dunque, sarebbe possibile compiere attività di *text and data mining*, a prescindere dallo scopo o dalla qualificazione del soggetto che compie detta attività; dunque anche per eventuali attività commerciali.

L’art. 4 della Direttiva Copyright, recepito nell’art. 70-quater nella normativa nazionale sulla LdA, assume un ruolo di rilievo nella presente disamina, in quanto possiamo scorgere lo stretto rapporto tra un sistema di intelligenza artificiale generativa, addestrata su materiale e banche dati di terzi attraverso il *text and data mining*, e il diritto esclusivo dei titolari su quel materiale e quelle banche dati.

Un ulteriore aspetto problematico concerne, poi, la conservazione delle copie dei dati, dopo che il *data mining* si è concluso. Il comma 2 dell’art. 70-quarter LdA prevede, infatti, che le riproduzioni e le estrazioni “*possono essere conservate per il tempo necessario ai fini dell’estrazione di testo e di dati”*. 

Pertanto, non sarebbe permessa la conservazione di copie da parte dei titolari di sistemi di AI generativa, per fini ulteriori rispetto a quello del *mining*. In tal caso, dunque, occorre verificare caso per caso se l’addestramento del sistema può configurarsi come una mera estrazione di testo e dati o se, invece, costituisca un’attività ad essa successiva e quindi eccedente rispetto ai limiti dell’art. 70-quarter LdA. 

Per concludere, sulla base dei punti analizzati, introdotti con la Direttiva Copyright e recepiti nella LdA, gli sviluppatori che intendono utilizzare opere di database protetti per addestrare un sistema di intelligenza artificiale generativa, dovrebbero riuscire a dimostrare:

1. di aver ottenuto un accesso legittimo al database:
1. di aver verificato che i titolari delle opere e del database non abbiano espressamente vietato le riproduzioni ai fini del *text and data mining* (c.d. meccanismo di *opt out*), così richiamando le attività di TDM al proprio controllo esclusivo. A tal riguardo, senza pretesa di approfondire tali aspetti nella presente sede, la portata liberalizzatrice del meccanismo di *opt out* dipenderà significativamente dalle modalità effettive con cui verrà effettuata la riserva da parte dei titolari*;*
1. di aver conservato le copie delle opere e del database solo per il tempo necessario ai fini del *text and data mining.*

Rispetto al meccanismo di *opt-out* sopra menzionato*,* è interessante il parere positivo fornito anche da* Communia[^17], la nota organizzazione internazionale di origine belga attiva nel promuove politiche di espansione del pubblico dominio e di incremento dell’accesso ed all’utilizzo della cultura e della conoscenza, che in un recente documento ha ritenuto l’approccio *opt-out* un quadro lungimirante per affrontare le questioni sollevate dall’utilizzo su larga scala di opere protette per l’addestramento di sistemi AI, in quanto garantisce un giusto equilibrio tra gli interessi dei titolari dei diritti da un lato, e dei ricercatori e degli sviluppatori dall’altro.

Venendo all’AI Act, emergono interessanti spunti dai recenti emendamenti introdotti dagli Europarlamentari al testo proposto dalla Commissione e dal Consiglio. Sebbene l’AI Act non sia specificamente destinato a mutare la disciplina del diritto d’autore, il Legislatore europeo sta infatti contemplando la possibilità di richiedere ai fornitori di sistemi di IA generativa maggior trasparenza e moderazione dei contenuti.  

Il punto centrale è indubbiamente quello introdotto dal comma 4 del novello articolo 28.b, relativo ai sistemi di intelligenza artificiale generativi, il cui obiettivo di fondo sembrerebbe quello di imporre, attraverso un’attività di controllo ed oneri preventivi, un certo grado di trasparenza in capo ai fornitori. 

Secondo il citato dispositivo, i fornitori di sistemi di AI generativa, descritti espressamente come “sistemi di AI specificamente destinati a generare, con vari livelli di autonomia, contenuti quali testi complessi, immagini, audio o video” sono tenuti a:

1) rispettare gli obblighi di trasparenza di cui all'articolo 52 comma 1 del medesimo AI Act;
1) addestrare e, se del caso, progettare e sviluppare il modello in modo da fornire adeguate garanzie contro la generazione di contenuti in violazione del diritto dell’Unione, in linea con lo stato dell’arte generalmente riconosciuto, e senza pregiudicare i diritti fondamentali, compresa la libertà di espressione;
1) fatta salva la legislazione nazionale o dell'Unione sul Diritto d’autore, documentare e rendere disponibile al pubblico una sintesi sufficientemente dettagliata dell’utilizzo dei dati di addestramento protetti dalla legge sul Diritto d’autore.

L’articolo conferma l’attenzione dei membri del Parlamento Europeo relativamente la citata analisi in tema di diritto d’autore e di *text and data mining*. Tuttavia, in assenza di maggiori precisazioni circa le modalità con cui i fornitori saranno tenuti ad adempiere agli obblighi di *clearance* imposti, in particolare per quel che attiene alla *“sintesi sufficientemente dettagliata dell’utilizzo dei dati di addestramento protetti dalla legge sul Diritto d’autore”*,  il perimetro della norma rischia di rimanere incerto, con la prospettiva – se il testo come riportato dovesse passare ad approvazione finale – di dover attendere un’eventuale posizione della Corte perché sia definita in concreto la portata degli obblighi del fornitore. 

Quel che è chiaro è che i requisiti di trasparenza per i fornitori saranno duplici.

In primo luogo, i fornitori dovranno rispettare gli obblighi di informazione di cui all’articolo 52, comma 1 e, in particolare, l’obbligo a che i sistemi di AI destinati a interagire con le persone fisiche siano progettati e sviluppati in modo tale da informare la persona fisica esposta a un sistema di AI che sta interagendo con un’intelligenza artificiale in modo tempestivo, chiaro e comprensibile, salvo che non emerga dalle circostanze e dal contesto di utilizzo. La disposizione dell’art. 52 aggiunge poi una serie di requisiti informativi aggiuntivi relativi all'interazione uomo-sistema AI, tra cui quello previsto al comma 3, in tema di “deep fake”: la norma imporrebbe all’utente del sistema, tramite il quale hanno generato dei contenuti, l’utilizzo di *disclaimer* appositi, per contrassegnare tutti quei contenuti di testo, audio o video non autentici, falsi o alterati, creati tramite il sistema stesso.

In secondo luogo, e salve le perplessità sollevate, i fornitori di modelli generativi di IA dovranno documentare e rendere disponibile al pubblico una sintesi dell’uso dei di cui all’art. 28b, comma 5 lett c). Questa è la disposizione che mira più distintamente a consentire l’*opt-out* ai sensi dell’articolo 4 della Direttiva Copyright in tema di *text and data mining*.


1

[^1]: https://obvious-art.com/
[^2]: https://www.christies.com/features/a-collaboration-between-two-artists-one-human-one-a-machine-9332-1.aspx
[^3]: https://www.wipo.int/wipo\_magazine/en/2017/05/article\_0003.html
[^4]: Legge 22 aprile 1941 . n. 633.
[^5]: V. Cass., civ. n.15496/2004; Cass., civ., n. 581/2007.
[^6]: Art. 25 LDA (L. 633/1941)
[^7]: Cass. Civ., sentenza n. 13171/2016
[^8]: i.e.U.S. 340/1991 - Feist Publications v Rural Telephone Service Company, Inc. 
[^9]: Australia, Full Federal Court of Appeal - Acohs Pty Ltd v Ucorp Pty Ltd (2012)
[^10]: C-5/08 - Infopaq International A/S v Danske Dagbaldes Forening)
[^11]: EWCA Civ 219 - Nova Productions v Mazooma Games (2007)
[^12]: https://www.govinfo.gov/content/pkg/FR-2023-03-16/pdf/2023-05321.pdf
[^13]: https://www.midjourney.com/home/
[^14]: https://discord.com/invite/midjourney
[^15]: Le immagini generate con piano gratuito vengono utilizzate da Midjourney come input per l’addestramento del software e sono visibili da tutti gli utenti. 
[^16]: https://openai.com/
[^17]: Policy Paper #15 on using copyrighted works for teaching the machine (communia-association.org)